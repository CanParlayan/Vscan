import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin


# Handling redirects should be explicit
# always changing user agent

class Crawler:
    @staticmethod
    def deep_crawl(start_url, max_depth):
        """
        Retrieve unique URLs up to specified depth
        Args:
            start_url: URL provided by user
            max_depth: Depth of scan

        Returns: list of unique URLs

        """
        visited = set()  # Keep track of visited URLs
        urls_to_visit = [(start_url, 1)]
        base_host = urlparse(start_url).netloc
        collected_urls = set()

        while urls_to_visit:
            url, current_depth = urls_to_visit.pop(0)

            # Skip if the depth is exceeded or the URL is already visited
            if current_depth > max_depth or url in visited:
                continue

            try:
                # Fetch the webpage content
                response = requests.get(url, timeout=5)
                soup = BeautifulSoup(response.content, 'html.parser')

                # Mark this URL as visited and add it to the collection
                visited.add(url)
                collected_urls.add(url)

                if current_depth == max_depth:
                    continue

                # Find and queue links on the page
                for link in soup.find_all('a', href=True):
                    href = link.get('href')
                    next_url = urljoin(url, href)

                    # Check if the URL belongs to the same host and has not been visited
                    if urlparse(next_url).netloc == base_host and next_url not in visited:
                        urls_to_visit.append((next_url, current_depth + 1))

            except requests.RequestException:
                continue

        return list(collected_urls)
